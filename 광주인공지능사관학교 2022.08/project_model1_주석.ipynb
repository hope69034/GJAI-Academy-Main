{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time # time() 함수 : 현재 Unix timestamp을 소수로 리턴,  정수부는 초단위이고 소수부는 마이크로 초단위\n",
        "import datetime # datetime.timedelta  : 기간을 표현하기 위해서 사용\n",
        "import copy # 복사\n",
        "import cv2\n",
        "import random # 랜덤\n",
        "import numpy as np \n",
        "import json # JSON(JavaScript Object Notation), attribute–value pairs / array data types / any other serializable value 로 이루어진 데이터를 전달하기 텍스트를 사용하는 포맷\n",
        "import torch # facebook에서 제공하는 딥러닝 도구, numpy와 효율적인 연동을 지원 \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt # 시각화 \n",
        "from PIL import Image # PIL 이미지 제어\n",
        "from efficientnet_pytorch import EfficientNet # EfficientNet : 이미지 분류 최고의 모델\n",
        "#변수 선언\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  \n",
        "    # torch.cuda.device(device) : 선택된 장치를 변경하는 context 관리자\n",
        "    # torch.cuda.device 의 파라미터 : device ( torch.device 또는 int ) – 선택할 장치 인덱스, 인수가 음의 정수 또는 None이면 작동X(no-op)\n",
        "hyper_param_batch = 6 # 배치 사이즈\n",
        "random_seed = 100 # 랜덤 시드\n",
        "# random_seed = 100 활용, 랜덤값 고정\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "# 변수 선언\n",
        "num_classes = 4 # 이피션트넷 모델선언 (파라미터)\n",
        "model_name = 'efficientnet-b7' # 진짜 모델 이름\n",
        "train_name = 'model1' # 트레인, 벨리, 테스트 셋 상위폴더 이름\n",
        "PATH = './scalp_weights/' # 경로 설정 현재폴더 하위에 scalp_weights 폴더\n",
        "data_train_path = './train_data/'+train_name+'/train' # 현재폴더/train_data/model1/train\n",
        "data_validation_path = './train_data/'+train_name+'/validation' # 현재폴더/train_data/model1/validation\n",
        "data_test_path = './train_data/'+train_name+'/test' # 현재폴더/train_data/model1/test\n",
        "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "image_size = EfficientNet.get_image_size(model_name) # model_name = 'efficientnet-b7'\n",
        "print(image_size) # 600 출력\n",
        "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "# 모델\n",
        "model = EfficientNet.from_pretrained( model_name, num_classes=num_classes ) # 이피션트넷 모델 선언 # pretrained weight 로드\n",
        "    # num_classes = 4 # 이피션트넷 모델선언 (파라미터)\n",
        "    # model_name = 'efficientnet-b7' # 진짜 모델 이름\n",
        "model = model.to(device) # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  \n",
        "# transforms.Compose : Rescale 과 RandomCrop 을 한번에 수행\n",
        "    # Rescale: 이미지의 크기를 조절 \n",
        "    # RandomCrop: 이미지를 무작위로 자른다\n",
        "    # 정규화\n",
        "transforms_train = transforms.Compose([\n",
        "                                        transforms.Resize([int(600), int(600)], interpolation=4),\n",
        "                                        transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                        transforms.RandomVerticalFlip(p=0.5),\n",
        "                                        transforms.Lambda(lambda x: x.rotate(90)),\n",
        "                                        transforms.RandomRotation(10),\n",
        "                                        transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "transforms_val = transforms.Compose([\n",
        "                                        transforms.Resize([int(600), int(600)], interpolation=4),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "# data_train_path 경로의 이미지를 transforms.Compose 로 정규화\n",
        "train_data_set = datasets.ImageFolder(data_train_path, transform=transforms_train)\n",
        "val_data_set = datasets.ImageFolder(data_validation_path, transform=transforms_val)\n",
        "# 변수 선언\n",
        "dataloaders, batch_num = {}, {}\n",
        "# dataloaders 빈딕셔너리에 train/val 키랑 DataLoder 밸류 넣기\n",
        "# DataLoader로 학습용 데이터 준비 : 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한다\n",
        "dataloaders['train'] = DataLoader(train_data_set,        \n",
        "                                  batch_size=hyper_param_batch,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=4)\n",
        "dataloaders['val'] = DataLoader(val_data_set,\n",
        "                                batch_size=hyper_param_batch,\n",
        "                                shuffle=False,\n",
        "                                num_workers=4)\n",
        "# 즉 dataloaders 딕셔너리에는 train / val 이 key 각 밸류는 정규화한 이미지 데이터에 + 라벨이 붙음\n",
        "# 배치_넘은 빈 딕셔너리\n",
        "# train/val 을 key로 각 밸류 선언\n",
        "batch_num['train'], batch_num['val'] = len(train_data_set), len(val_data_set)\n",
        "print('batch_size : %d,  train/val : %d / %d' % (hyper_param_batch, batch_num['train'], batch_num['val']))\n",
        "# 출력 :      batch_size : 6,   train/val :             2066       /         499\n",
        "#      hyper_param_batch = 6    train/val :  len(train_data_set)   /    len(val_data_set)\n",
        "class_names = train_data_set.classes  # train_data_set 정규화한 트레인셋\n",
        "print(class_names) # 출력 : [ '[원천]미세각질_0.양호', '[원천]미세각질_1.경증', '[원천]미세각질_2.중등도', '[원천]미세각질_3.중증'  ]\n",
        "# def 선언 후 마지막에 train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs) 로 실행\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    ## 변수 선언\n",
        "    # 시간변수 선언\n",
        "    start_time = time.time() # end_sec 종료시간 = time.time() - start_time, # 종료시간 : \n",
        "    since = time.time() # time_elapsed 경과시간 = time.time() - since, # 경과시간 : 모든 에폭을 돌리는데 걸린 시간\n",
        "    best_acc = 0.0 # 베스트 정확도 갱신시킬 변수\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # 베스트가중치도 갱신: 베스트 정확도 갱신할 때 같이 갱신\n",
        "# state_dict 는 간단히 말해 각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체입니다.\n",
        "# state_dict : 모델의 매개변수를 딕셔너리로 저장\n",
        "# copy.deepcopy 깊은복사: 완전한복사 (얕은복사:일종의 링크 형태)\n",
        "    # 손실, 정확도 빈리스트 선언\n",
        "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
        "    # for문 \n",
        "    for epoch in range(num_epochs): # epoch만큼 실행\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1)) # 1000에폭을 넣으면 Epoch 0/999 이렇게 출력      왜 -1을 넣었을가 ?\n",
        "        print('-' * 10) # 그저 ---------- 구분 선\n",
        "        epoch_start = time.time() # 매 에폭을 돌리는 시간 \n",
        "        for phase in ['train', 'val']:   \n",
        "            if phase == 'train': \n",
        "                model.train() # model.train()  ≫ 모델을 학습 모드로 변환\n",
        "            else:\n",
        "                model.eval() # model.eval()  ≫ 모델을 평가 모드로 변환\n",
        "            # train이 들어가면 학습모드로 아래 코드 실행, val이 들어가면 평가모드로 val로 평가\n",
        "            # 변수\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            num_cnt = 0\n",
        "# 아래코드이해를위한\n",
        "# dataloaders 빈딕셔너리에 train/val 키랑 DataLoder 밸류 넣기\n",
        "# DataLoader로 학습용 데이터 준비 : 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한다\n",
        "# dataloaders['train'] = DataLoader(train_data_set,        \n",
        "#                                   batch_size=hyper_param_batch,\n",
        "#                                   shuffle=True,\n",
        "#                                   num_workers=4)\n",
        "# dataloaders['val'] = DataLoader(val_data_set,\n",
        "#                                 batch_size=hyper_param_batch,\n",
        "#                                 shuffle=False,\n",
        "#                                 num_workers=4)\n",
        "            for inputs, labels in dataloaders[phase]: # phase 에 train or val 이 들어가서 인풋과 라벨로 나뉜다\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad() # optimizer.zero_grad() : Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문\"에 \n",
        "                                      # 우리는 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다.\n",
        "                                      # 한번 학습이 완료가 되면 gradients를 0으로 초기화\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "# torch.set_grad_enabled\n",
        "# 그래디언트 계산을 켜키거나 끄는 설정을 하는 컨텍스트 관리자\n",
        "# phase == 'train' 이 true 면 gradients를 활성화 한다.\n",
        "                    outputs = model(inputs) # 모델에 인풋을 넣어서 아웃풋 생성\n",
        "                    _, preds = torch.max(outputs, 1)  # _, preds ?\n",
        "                    # torch.max(input-tensor) : 인풋에서 최댓값을 리턴하는데 tensor라 각 묶음마다 최댓값을 받고 ,1 은 축소할 차원이1이라는 뜻\n",
        "                    loss = criterion(outputs, labels) # 로스 계산\n",
        "# 매 epoch, 매 iteration 마다 back propagation을 통해 모델의 파라미터를 업데이트 시켜주는 과정이 필요한데, \n",
        "# 아래 다섯 줄의 코드는 공식처럼 외우는 것을 추천드립니다.\n",
        "# optimizer.zero_grad()   \t# init grad\n",
        "# pred = model(x)  # forward\n",
        "# loss = criterion(pred, x_labels) # 로스 계산\n",
        "# loss.backward()  # backpropagation\n",
        "# optimizer.step()  \t# weight update\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()  \t# backpropagation\n",
        "                        optimizer.step()   \t# weight update\n",
        "                running_loss += loss.item() * inputs.size(0)  \t# 학습과정 출력   #   running_loss = 0.0    # loss 는 로스계산  ?\n",
        "                running_corrects += torch.sum(preds == labels.data)               #   running_corrects = 0                    ?\n",
        "                num_cnt += len(labels)                                            #   num_cnt = 0                             ?\n",
        "# for inputs, labels in dataloaders[phase]: # phase 에 train or val 이 들어가서 인풋과 라벨로 나뉜다\n",
        "#                 inputs = inputs.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "            if phase == 'train':\n",
        "                scheduler.step() # 학습 규제\n",
        "# 학습률이 크면 가중치 업데이트가 많아 가중치가 overflow 될 수도 있습니다\n",
        "# 훈련 초기에 학습률은 충분히 좋은 가중치에 도달하기 위해 크게 설정됩니다. 시간이 지남에 따라 \n",
        "# 이러한 가중치는 작은 학습률을 활용하여 더 높은 정확도에 도달하도록 미세 조정됩니다.\n",
        "# 결국, 가중치를 규제(regularization)하는 방식과 비슷하게, 학습률을 규제하는(Learning Rate Decay)것이 Learning Rate Scheduler라고 할 수 있습니다.\n",
        "# optimizer와 scheduler를 먼저 정의한 후, 학습할 때 batch마다 optimizer.step() 하고 epoch마다 scheduler.step()을 해주면 됩니다. \n",
        "# def 밖에서  op sc 선언 def안에서 op.step  sc.stop 완료\n",
        "            epoch_loss = float(running_loss / num_cnt) # ? 에폭손실\n",
        "            epoch_acc = float((running_corrects.double() / num_cnt).cpu() * 100) # ? 에폭 정확도\n",
        "#     손실, 정확도 빈리스트 선언\n",
        "#    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "            else:\n",
        "                val_loss.append(epoch_loss)\n",
        "                val_acc.append(epoch_acc)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) # 출력 train/val, 손실, 정확도\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_idx = epoch # 에폭인덱\n",
        "                best_acc = epoch_acc # 베스트정확도\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) \n",
        "                print('==> best model saved - %d / %.1f' % (best_idx, best_acc))  # 몇번째 에폭의 베스트 정확도가 세이브되었나 출력\n",
        "#     best_acc = 0.0 # 베스트 정확도 갱신시킬 변수\n",
        "#     best_model_wts = copy.deepcopy(model.state_dict()) # 베스트가중치도 갱신: 베스트 정확도 갱신할 때 같이 갱신\n",
        "# state_dict 는 간단히 말해 각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체입니다.\n",
        "# state_dict : 모델의 매개변수를 딕셔너리로 저장\n",
        "# copy.deepcopy 깊은복사: 완전한복사 (얕은복사:일종의 링크 형태)\n",
        "            epoch_end = time.time() - epoch_start # train/val 전부 에폭 한번 돌리는 시간을 구해서 아래 출력\n",
        "            print('Training epochs {} in {:.0f}m {:.0f}s'.format(epoch, epoch_end // 60, epoch_end % 60)) #트레이닝에폭 epoch 몇분 몇초\n",
        "            print()\n",
        "            # for문 끝\n",
        "    time_elapsed = time.time() - since # 경과시간 : 모든 에폭을 돌리는데 걸린 시간, for 문이 끝났으니까\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) # 경과시간을 몇분 몇초로 출력\n",
        "    print('Best valid Acc: %d - %.1f' % (best_idx, best_acc)) # best_idx : 몇번째 에폭이 베스트인지, 베스트정확도 출력\n",
        "    model.load_state_dict(best_model_wts) # state_dict: 모델의 매개변수를 딕셔너리에 담은 > 것을 load 한다\n",
        "# best_model_wts = copy.deepcopy(model.state_dict()) \n",
        "# PATH = './scalp_weights/' # 경로 설정 현재폴더 하위에 scalp_weights 폴더\n",
        "    torch.save(model, PATH + 'aram_' + train_name + '.pt') # 모델을 PATH경로에 aram~.pt 파일로 저장\n",
        "    torch.save(model.state_dict(), PATH + 'president_aram_' + train_name + '.pt') # 모델의 매개변수를 PATH경로에 presi~.pt 파일로 저장\n",
        "    print('model saved')\n",
        "    end_sec = time.time() - start_time # 종료시간    # 초단위에서 \n",
        "    end_times = str(datetime.timedelta(seconds=end_sec)).split('.')  # 시분초로 치환\n",
        "# import datetime\n",
        "# end = 8888\n",
        "# datetime.timedelta(seconds=end)                                #출력  datetime.timedelta(seconds=8888)\n",
        "# str(datetime.timedelta(seconds=end))  # type str              #출력  '2:28:08'\n",
        "# str(datetime.timedelta(seconds=end)).split('.') # type list   #출력   ['2:28:08']  ?\n",
        "# str(datetime.timedelta(seconds=end)).split('.')[0] # type str  #출력  '2:28:08'\n",
        "    end_time = end_times[0] # 종료시간 시분초\n",
        "    print(\"end time :\", end_time) # 출력\n",
        "    return model, best_idx, best_acc, train_loss, train_acc, val_loss, val_acc \n",
        "    # def 문 끝\n",
        "# def실행할 train_model 파라미터 선언\n",
        "model = EfficientNet.from_pretrained( model_name, num_classes=num_classes ).to(device) # 첫번째 파라미터 (위에서 선언)\n",
        "criterion = nn.CrossEntropyLoss() # 두번째 파라미터\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4) # 세번째 파라미터\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) # 네번째 파라미터  # 스케줄러 선언\n",
        "num_epochs = 1000  # 다섯번째 파라미터\n",
        "# def문 실행 \n",
        "train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs)      "
      ],
      "metadata": {
        "id": "TBSD605_NS5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
