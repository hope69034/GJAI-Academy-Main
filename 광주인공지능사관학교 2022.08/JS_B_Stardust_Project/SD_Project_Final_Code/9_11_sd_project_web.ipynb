{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quI-zg8VE0z6",
        "outputId": "0e764de8-9429-4084-d4ca-6cbe7eacb7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDUkKR2jFYoJ"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/project/static/ngrok_auth.txt') as nf:\n",
        "    ngrok_auth = nf.read()\n",
        "#!cat drive/MyDrive/project/static/ngrok_auth.txt \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuB0g_RYGXpx",
        "outputId": "6b030fee-399c-40c0-ce4a-0ad9daecc129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (4.1.1)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=e4d8ca807681cf2bbc235c8fcb33a42967e36c951fc1742c1dfe32615ea94787\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ],
      "source": [
        "#설치, ngrok read\n",
        "!pip install flask-ngrok > /dev/null 2>&1\n",
        "!pip install pyngrok > /dev/null 2>&1\n",
        "!ngrok authtoken $ngrok_auth\n",
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3jBh_hqI2Wh"
      },
      "outputs": [],
      "source": [
        "\n",
        "#from, import\n",
        "from flask import Flask, render_template, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "#웹- 업로드 시 실행하지 않게 미리 임폴트\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EHzD9kdFiCU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#웹페이지에서 이미지를 업로드 받은 후 연산시간을 줄이기 위해 미리 로드하는 것\n",
        "#PATH\n",
        "PATH1 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model1.pt'  # 모델1\n",
        "PATH2 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model2.pt'  # 모델2\n",
        "PATH3 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model3.pt'  # 모델3\n",
        "PATH4 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model4.pt'  # 모델4\n",
        "PATH5 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model5.pt'  # 모델5\n",
        "PATH6 = '/content/drive/MyDrive/project/scalp_weights/'+'aram_model6.pt'  # 모델6\n",
        "#cuda # 쿠다를 쓸 수 있다면 쿠다 아니면 시피유        \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model load        \n",
        "model1 = torch.load(PATH1, map_location=device)\n",
        "model2 = torch.load(PATH2, map_location=device)\n",
        "model3 = torch.load(PATH3, map_location=device)\n",
        "model4 = torch.load(PATH4, map_location=device)\n",
        "model5 = torch.load(PATH5, map_location=device)\n",
        "model6 = torch.load(PATH6, map_location=device)\n",
        "#아웃풋, 로스, 프레딕, 아큐러시\n",
        "#모델 평가모드로 전환 # 평가모드와 학습모드의 layer 구성이 다르다\n",
        "model1.eval() \n",
        "model2.eval()\n",
        "model3.eval()\n",
        "model4.eval()\n",
        "model5.eval()\n",
        "model6.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y55dThWh-hTU",
        "outputId": "92b8f5bf-1142-4eb7-cf89-3ba232fb9c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://477f-35-243-138-109.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2022 04:37:12] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2022 04:37:13] \"\u001b[37mGET /static/img/stardust.jpg HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2022 04:37:14] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Sep/2022 04:37:15] \"\u001b[37mGET /menu HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##Web Server Code\n",
        "app = Flask(__name__, static_folder='/content/drive/MyDrive/project/static', # 이곳에 유저가 업로드한 파일을 save\n",
        "                      template_folder='/content/drive/MyDrive/project/templates')\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "def home():\n",
        "    menu = {'home':1, 'menu':0}\n",
        "    return render_template('index.html', menu=menu, )     \n",
        "@app.route('/menu', methods=['GET','POST'])\n",
        "def menu():\n",
        "    menu = {'home':0, 'menu':1}\n",
        "    if request.method == 'GET':\n",
        "        return render_template('menu.html', menu=menu)   # 서버에서 클라이언트로 정보 전달\n",
        "    else:\n",
        "        f_image = request.files['image']\n",
        "        fname = f_image.filename                # 사용자가 입력한 파일 이름\n",
        "        newname = 'upload.jpg' # 업로드폴더를리셋하기위해일정한새이름을지정 upload.jpg로 업로드\n",
        "        filename = os.path.join(app.static_folder, 'upload/upload/') + newname  # 유저가 업로드한 사진이 저장되는 공간과 파일이름 \n",
        "                                        #static_folder/upload 가 경로 파일네임 : fname 유저가 업로드한 파일의 이름 \n",
        "        f_image.save(filename) #파일세이브\n",
        "        return render_template('menu_spinner.html', menu=menu ,  filename=filename )\n",
        "        #fname=newname  ,  mtime=mtime,       \n",
        "@app.route('/menu_res', methods=['POST'])\n",
        "def menu_res():\n",
        "    menu = {'home':0, 'menu':1}\n",
        "#########################################################################\n",
        "##model.test code\n",
        "#test 이미지파일 전처리, 텐서화\n",
        "    #전처리-트랜스폼 규칙 선언 # model1_train 코드의 validation set 의 트랜스폼 규칙과 동일하게 \n",
        "    transforms_test = transforms.Compose([  transforms.Resize([int(600), int(600)], interpolation=transforms.InterpolationMode.BOX),\n",
        "                                            transforms.ToTensor(), #텐서화\n",
        "                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])       #노말라이즈 정규화                            \n",
        "    #root 경로 폴더 속 jpg를 전처리, 텐서화 (rood 속에 폴더를 하나 더 만들어서 jpg를 묶어야 함)\n",
        "    testset = torchvision.datasets.ImageFolder(root = '/content/drive/MyDrive/project/static/upload' ,\n",
        "                        transform = transforms_test)\n",
        "#DataLoader를 통해 네트워크에 올리기 \n",
        "    testloader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)\n",
        "    #데이터 로더는 데이터의 대량 가져오기 또는 내보내기를 위한 클라이언트 응용 프로그램 \n",
        "    #for data, target in testloader: 에서 data는 데이터의 특징  target은 데이터의 정답값\n",
        "#if __name__ == '__main__':\n",
        "    with torch.no_grad(): # 평가할 땐  gradient를 backpropagation 하지 않기 때문에 no grad로 gradient 계산을 막아서 연산 속도를 높인다\n",
        "        for data, target in testloader:                                   \n",
        "            data  = data.to(device) # data, target  = data.to(device), target.to(device) \n",
        "            output1 = model1(data)   # model1에 데이터를 넣어서 아웃풋 > [a,b,c,d] 각 0,1,2,3 의 확률값 리턴 가장 큰 것이 pred\n",
        "            output2 = model2(data) \n",
        "            output3 = model3(data) \n",
        "            output4 = model4(data) \n",
        "            output5 = model5(data) \n",
        "            output6 = model6(data)  \n",
        "#predict # # 0~3값만 뽑기 \n",
        "    m1p = output1.argmax(dim=1, keepdim=True)[0][0].tolist() # argmax로 최댓값의 인덱스 뽑기\n",
        "    m2p = output2.argmax(dim=1, keepdim=True)[0][0].tolist()\n",
        "    m3p = output3.argmax(dim=1, keepdim=True)[0][0].tolist()\n",
        "    m4p = output4.argmax(dim=1, keepdim=True)[0][0].tolist()\n",
        "    m5p = output5.argmax(dim=1, keepdim=True)[0][0].tolist()\n",
        "    m6p = output6.argmax(dim=1, keepdim=True)[0][0].tolist()\n",
        "#진단\n",
        "    d_list = [] # 두피유형진단결과\n",
        "    #두피 유형 진단법\n",
        "    if m1p < 1  and m2p < 1 and m3p < 1 and m4p < 1 and m5p < 1 and m6p < 1 :\n",
        "        d1 = '정상입니다.'\n",
        "        d_list.append(d1)\n",
        "    elif m1p >= 1 and m2p < 1 and m3p < 1 and m4p < 1 and m5p < 1 and m6p < 1 :\n",
        "        d2 = '건성 두피입니다.' \n",
        "        d_list.append(d2)\n",
        "    elif m1p < 1 and m2p >=1  and m3p < 1 and m4p < 1 and m5p < 1 and m6p < 1 :\n",
        "        d3 = '지성 두피입니다.'\n",
        "        d_list.append(d3)\n",
        "    elif m2p < 1 and m3p >= 1 and m4p < 1 and m5p < 1  and m6p < 1 :\n",
        "        d4 = '민감성 두피입니다.'\n",
        "        d_list.append(d4)\n",
        "    elif m2p >= 1 and m3p >= 1 and m4p < 1 and m6p < 1 :\n",
        "        d5 = '지루성 두피입니다.'\n",
        "        d_list.append(d5)\n",
        "    elif m3p < 1 and m4p  >= 1 and m6p < 1 :\n",
        "        d6 = '염증성 두피입니다.'\n",
        "        d_list.append(d6)\n",
        "    elif m3p < 1 and m4p < 1 and m5p >= 1 and m6p < 1 :\n",
        "        d7 = '비듬성 두피입니다.'\n",
        "        d_list.append(d7)\n",
        "    elif m1p < 1 and m2p >= 1 and m3p < 1 and m4p < 1 and m5p < 1 and m6p >= 1 :\n",
        "        d8 = '탈모입니다.'\n",
        "        d_list.append(d8)\n",
        "    else:\n",
        "        d9 = '복합성 두피입니다.'\n",
        "        d_list.append(d9)\n",
        "#########################################################################\n",
        "##Web Server Code\n",
        "#모델 실행후 결과를 돌려줌\n",
        "    final = d_list[0] # 두피유형판단\n",
        "    result = {'미세각질':m1p, '피지과다':m2p,'모낭사이홍반':m3p,'모낭홍반농포':m4p,'비듬':m5p,'탈모':m6p} # result=result\n",
        "    final2 = '0:양호, 1:경증, 2:중등도, 3:중증' \n",
        "    filename = request.args['filename']  #html에서 넘겨준 파일네임을 리퀘스트로 불러오기\n",
        "    mtime = int(os.stat(filename).st_mtime) # 업로드한 시간값불러오기 > 큐변경 > 화면갱신 4\n",
        "    return render_template('menu_res.html',  final2=final2,final=final, menu=menu, result=result, \n",
        "                            m1p=m1p,m2p=m2p,m3p=m3p,m4p=m4p,m5p=m5p,m6p=m6p , mtime=mtime    )\n",
        "#solution 화면            \n",
        "@app.route('/menu_res_inf')\n",
        "def menu_res_inf(): \n",
        "    menu = {'home':0, 'menu':0}\n",
        "    m1p = int(request.args['m1p']) # menu_res에서 m1p 를 가져오기\n",
        "    m2p = int(request.args['m2p'])\n",
        "    m3p = int(request.args['m3p'])\n",
        "    m4p = int(request.args['m4p'])\n",
        "    m5p = int(request.args['m5p'])\n",
        "    m6p = int(request.args['m6p'])\n",
        "    return render_template('menu_res_inf.html', menu=menu, m1p=m1p,m2p=m2p,m3p=m3p,m4p=m4p,m5p=m5p,m6p=m6p, )\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}