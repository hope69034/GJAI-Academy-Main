{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beautiful soup",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# beautiful soup"
      ],
      "metadata": {
        "id": "fk4sJtpIsqv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HTML 파일열기\n",
        "from bs4 import BeautifulSoup\n",
        "with open(\"00_Example.html\") as fp:\n",
        "    soup = BeautifulSoup(fp, 'html.parser')"
      ],
      "metadata": {
        "id": "HPVbhgAftxyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## urllib 를 통해서 웹에 있는 소스 가져오기\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "\n",
        "# web_url 에 원하는 웹의 URL을 넣어주면 됨\n",
        "with urllib.request.urlopen(유알엘) as response:\n",
        "    html = response.read()\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# 단순히 아래와 같이 해도 됨\n",
        "from urllib.request import urlopen\n",
        "html = urlopen(유알엘)\n",
        "soup = BeautifulSoup(html, 'html.parser')\n"
      ],
      "metadata": {
        "id": "Z1gl6VODuSRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## requests 이용\n",
        "import requests\n",
        "url = 'https://www.melon.com/chart/week/index.htm'\n",
        "header = {'User-Agent':\n",
        "          'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
        "req = requests.get(url, headers = header)\n",
        "html = req.text\n",
        "\n",
        "# 개발자 콘솔에서 navigator.userAgent로 확인\n",
        "\"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Mobile Safari/537.36\""
      ],
      "metadata": {
        "id": "0JQERVF7v4ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 태그를 이용해서 가져왹\n",
        "## find 해당 조건에 맞는 하나의 태그를 가져온다. 중복이면 가장 첫 번째 태그를 가져온다\n",
        "from bs4 import BeautifulSoup\n",
        "fp = open(\"00_Example.html\")\n",
        "soup = BeautifulSoup(fp, 'html.parser')\n",
        "\n",
        "first_div = soup.find(\"dib\")\n",
        "print(first_div)\n",
        "\n",
        "# 출력 결과\n",
        "# <div>\n",
        "# <p>a</p>\n",
        "# <p>b</p>\n",
        "# <p>c</p>\n",
        "# </div>"
      ],
      "metadata": {
        "id": "VtbWyyPZw6dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## find_all - 해당 조건에 맞는 모든 태그들을 가져온다\n",
        "all_divs = soup.find_all(\"div\")\n",
        "print(all_divs)\n",
        "\n",
        "# 출력결과\n",
        "# [<div> \n",
        "# <p>a</p> \n",
        "# <p>b</p>\n",
        "# <p>c</p>\n",
        "# </div>, <div class=\"ex_class\">\n",
        "# <p>1</p>\n",
        "# <p>2</p>\n",
        "# <p>3</p>\n",
        "# </div>, <div id=\"ex_id\">\n",
        "# <p>X</p>\n",
        "# <p>Y</p>\n",
        "# <p>Z</p>\n",
        "# </div>\n",
        "\n",
        "# 동일한 결과\n",
        "딕셔너리 대체\n",
        "    result = soup.find('div', id = 'ex_id')"
      ],
      "metadata": {
        "id": "mu3kMOCZxy7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## class 이름으로 찾기\n",
        "result = soup.find('div', {'class': 'ex_class'})\n",
        "print(result)\n",
        "\n",
        "# 출력 결과\n",
        "# <div class=\"ex_class\">\n",
        "# <p>1</p>\n",
        "# <p>2</p>\n",
        "# <p>3</p>\n",
        "# </div>\n",
        "\n",
        "## 동일한 결과를 얻는 방법\n",
        "1. class 생략\n",
        "    result = soup.find('div', 'ex_class')\n",
        "2. class_ (under bar가 있음) 로 검색\n",
        "    result = soup.find(class_ = 'ex_class')"
      ],
      "metadata": {
        "id": "5Lq2qSYyzBbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CSS Selector 로 찾기\n",
        "1. id로 찾기 \n",
        "    result = soup.select_one('#ex_id')\n",
        "    result_list = soup.select('#ex_id')\n",
        "2. class로 찾기\n",
        "    result = soup.select_one('.ex_class')\n",
        "    result_list = soup.select('.ex_class')"
      ],
      "metadata": {
        "id": "psxXSJ_kxIj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 결과 가져오기\n",
        "ex_id_divs = soup.find(\"div\", {\"id\":\"ex_id\"})\n",
        "# 찾은 태그들 안에서 p태그를 가져온다.\n",
        "all_ps_in_ex_id_divs = ex_id_divs.find_all(\"p\")\n",
        "print(all_ps_in_ex_id_divs)\n",
        "\n",
        "# 출력 결과\n",
        "[ <p>X</p>, <p>Y</p>, <p>Z</p>]\n",
        "\n",
        "all_ps_in_ex_id_divs[0].git_text()\n",
        "X\n",
        "all_ps_in_ex_id_divs[1].git_text()\n",
        "Y"
      ],
      "metadata": {
        "id": "gV56lG7I0ay-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## attribute 결과 가져오기\n",
        "<a href = \"www.naver.com\" class = \"a\">Naver</a>\n",
        "\n",
        "a_tag = soup.find('a')\n",
        "\n",
        "a_tag.string\n",
        "Naver\n",
        "\n",
        "a_tag.attrs['href']\n",
        "www.naver.com\n",
        "\n",
        "a_tag['href']\n",
        "www.naver.com"
      ],
      "metadata": {
        "id": "jMAQ01gN1YaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}