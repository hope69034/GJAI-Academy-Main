{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLBSMKgtSQn7",
        "outputId": "652ea7b1-c4fa-4049-b252-451ca74df733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.02391914]\n",
            " [0.9757925 ]\n",
            " [0.97343127]\n",
            " [0.03041428]]\n",
            "[[False]\n",
            " [ True]\n",
            " [ True]\n",
            " [False]]\n"
          ]
        }
      ],
      "source": [
        "# XOR class로 구현\n",
        "# 입력층노드3개 -  은닉층노드6개 - 출력층노드1개\n",
        "\n",
        "import numpy as np\n",
        "#뉴런은노드의수로가중치의개수가결정된다\n",
        "# 3 6 1\n",
        "\n",
        "# 시그모이드 함수\n",
        "def actf(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# 시그모이드 함수의 미분값\n",
        "def actf_deriv(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# XOR 연산을 위한 4행*2열의 입력 행렬\n",
        "# 마지막 열[1]은 바이어스값을 나타낸다.\n",
        "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "\n",
        "# XOR 연산을 위한 4행*1열의 목표 행렬 # 0인지 1인지 출력이 1개니까 한개\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "np.random.seed(5)\n",
        "\n",
        "inputs = 3  # 입력층의 노드 개수\n",
        "hiddens = 6  # 은닉층의 노드 개수\n",
        "outputs = 1  # 출력층의 노드 개수\n",
        "\n",
        "# 가중치를 –1.0에서 1.0 사이의 난수로 초기화한다.\n",
        "weight0 = 2 * np.random.random((inputs, hiddens)) - 1 #입력층과 은닉층 사이의 웨이트\n",
        "weight1 = 2 * np.random.random((hiddens, outputs)) - 1  #은닉층과 출력층 사이의 웨이트\n",
        "\n",
        "# 반복한다.\n",
        "for i in range(10000): # 백프로파게이션 자꾸 반복~\n",
        "    # 순방향 계산           레이어0 입력층 , 레이어1: 은닉층 , 레이어2: 출력층\n",
        "    layer0 = X  # 입력을 layer0에 대입한다.\n",
        "    net1 = np.dot(layer0, weight0)  # 행렬의 곱을 계산한다.       #은닉층으로 들어갈 인풋에 웨이트 적용\n",
        "    layer1 = actf(net1)  # 활성화 함수를 적용한다.                #은닉층에서 액티베이션펑션 적용\n",
        "    layer1[:, -1] = 1.0  # 마지막 열은 바이어스를 나타낸다. 1.0으로 만든다.\n",
        "    net2 = np.dot(layer1, weight1)  # 행렬의 곱을 계산한다        #은닉층에서 액티베이션펑션 적용되어 나온 것에 웨이트 적용\n",
        "    layer2 = actf(net2)  # 활성화 함수를 적용한다.                #출력층에서 액티베이션펑션 적용 > 출력층아웃풋\n",
        "    \n",
        "    #layer2는프레딕트값이니까 아래 오차를 계산\n",
        "\n",
        "\n",
        "##백프로파게이션과정\n",
        "\n",
        "#로스게산\n",
        "    # 출력층에서의 오차를 계산한다.\n",
        "# layer2에러 : 은닉층과 출력층 사이의 오차값\n",
        "    layer2_error = layer2 - y\n",
        "\n",
        "    #가중치의갱신을위해역으로돌아간다 : 오차 역전파  백프로파게이션   오차역전파를 통해 가중치를 업데이트해서 학습\n",
        "\n",
        "    #미분을하면줄어든다-많이하면-문제가생긴다-딥러닝의학습문제-(층이깊어지면문제 레이어를거치다\n",
        "    #계속줄어서 사라진다 오차역전파가 전달이 안된다) 이걸보완하는법-\n",
        "    #뒤로갈때 미분을 사용\n",
        "\n",
        "#액티베이션펑션을미분한것으로뒤로가기\n",
        "    # 출력층에서의 델타값을 계산한다.\n",
        "    layer2_delta = layer2_error * actf_deriv(layer2)   # actf_deriv은 액티베이션펑션을 미분한 것 \n",
        "   #레이어2델타로 아래서                                          #이게 미분\n",
        "        #델타:오차곱하게엑티베이션평션미분~~~~\n",
        "\n",
        "    # 은닉층에서의 오차를 계산한다.\n",
        "    # 여기서 T는 행렬의 전치를 의미한다.\n",
        "    # 역방향으로 오차를 전파할 때는 반대방향이므로 행렬이 전치되어야 한다.\n",
        "\n",
        "# layer1에러 : 입력층과 은닉층 사이의 오차값\n",
        "    layer1_error = np.dot(layer2_delta, weight1.T)\n",
        "                    #내적    입력층과 은닉층 사이의 오차값\n",
        "#   입력층 -  은닉층  -  출력층\n",
        "#         오차1     오차2\n",
        "#두가지오차가 아래서 갱신된다 weight\n",
        "\n",
        "    # 은닉층에서의 델타를 계산한다.\n",
        "    layer1_delta = layer1_error * actf_deriv(layer1)\n",
        "\n",
        "\n",
        "    # 은닉층->출력층을 연결하는 가중치를 수정한다.\n",
        "    weight1 += -0.2 * np.dot(layer1.T, layer2_delta)  # 웨이트1은 은닉층과 출력층 사이의 웨이트,  레이어1은 은닉층 레이어2는 출력층\n",
        "\n",
        "    # 입력층->은닉층을 연결하는 가중치를 수정한다.\n",
        "    weight0 += -0.2 * np.dot(layer0.T, layer1_delta) # 웨이트0은 입력층과 은닉층 사이의 웨이트\n",
        "print(layer2)  # 현재 출력층의 값을 출력한다.\n",
        "print(layer2>0.5)"
      ]
    }
  ]
}